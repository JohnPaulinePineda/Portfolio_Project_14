---
title: "R : Cost-Sensitive Learning for Severe Class Imbalance"
author: "John Pauline Pineda"
date: "December 13, 2022"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
| This document presents a non-exhaustive list of cost-sensitive learning procedures applied for severe class imbalance using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>. 
|
| Cost-sensitive learning and class weights address imbalanced classification problems when the cost of missing a minority class case might be different than incorrectly classifying an example from the majority class. In cost-sensitive learning, instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost. Thus, instead of trying to optimize the accuracy, the problem is then to minimize the total misclassification cost. Meanwhile, class weights are adjustments made to reduce the bias towards the most occurring categories in the data and compensate for the underrepresentation of the minority class by increasing the importance on correctly identifying examples from this group. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**caret**</mark> package attempt to learn models on imbalanced data that have uneven penalties or costs when making predictions.
|
##  1.1 Sample Data
|
| The <mark style="background-color: #EEEEEE;color: #FF0000">**Sonar**</mark>  dataset from the  <mark style="background-color: #CCECFF">**mlbench**</mark> package was used for this illustrated example. The original dataset was transformed to simulate class imbalance.
|
| Preliminary dataset assessment:
|
| **[A]** 136 rows (observations)
|      **[A.1]** Train Set = 96 observations with class ratio of 80:20
|      **[A.2]** Test Set = 40 observations with class ratio of 80:20
| 
| **[B]** 61 columns (variables)
|      **[B.1]** 1/61 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=R</span> < <span style="color: #FF0000">Class=M</span>
|      **[B.2]** 60/61 predictors = All remaining variables (60/60 numeric)
|     
| 
```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(lattice)
library(dplyr)
library(tidyr)
library(moments)
library(skimr)
library(RANN)
library(mlbench)
library(pls)
library(corrplot)
library(tidyverse)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stats)
library(nnet)
library(elasticnet)
library(earth)
library(party)
library(kernlab)
library(randomForest)
library(Cubist)
library(pROC)
library(mda)
library(klaR)
library(pamr)
library(MLmetrics)

##################################
# Loading source and
# formulating the train set
##################################
data(Sonar)

Sonar.Original <- Sonar

Sonar.M <- Sonar[Sonar$Class=="M",]
Sonar.R <- Sonar[Sonar$Class=="R",]
set.seed(12345678)
Sonar.R.Reduced <- Sonar.R[sample(1:nrow(Sonar.R),25),]

Sonar <- as.data.frame(rbind(Sonar.M,Sonar.R.Reduced))

set.seed(12345678)
Sonar_Partition <- createDataPartition(Sonar$Class, p = .70, list = FALSE)
Sonar_Train <- Sonar[Sonar_Partition,]
Sonar_Test  <- Sonar[-Sonar_Partition,]

##################################
# Performing a general exploration of the train set
##################################
dim(Sonar_Train)
str(Sonar_Train)
summary(Sonar_Train)

##################################
# Performing a general exploration of the test set
##################################
dim(Sonar_Test)
str(Sonar_Test)
summary(Sonar_Test)

##################################
# Formulating a data type assessment summary
##################################
PDA <- Sonar_Train
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```
##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** Low variance observed for 17 variables with First.Second.Mode.Ratio>5.
|      **[B.1]** <span style="color: #FF0000">V9</span> variable (numeric)
|      **[B.2]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[B.3]** <span style="color: #FF0000">V12</span> variable (numeric)
|      **[B.4]** <span style="color: #FF0000">V16</span> variable (numeric)
|      **[B.5]** <span style="color: #FF0000">V19</span> variable (numeric)
|      **[B.6]** <span style="color: #FF0000">V26</span> variable (numeric)
|      **[B.7]** <span style="color: #FF0000">V28</span> variable (numeric)
|      **[B.8]** <span style="color: #FF0000">V32</span> variable (numeric)
|      **[B.9]** <span style="color: #FF0000">V34</span> variable (numeric)
|      **[B.10]** <span style="color: #FF0000">V35</span> variable (numeric)
|      **[B.11]** <span style="color: #FF0000">V37</span> variable (numeric)
|      **[B.12]** <span style="color: #FF0000">V38</span> variable (numeric)
|      **[B.13]** <span style="color: #FF0000">V41</span> variable (numeric)
|      **[B.14]** <span style="color: #FF0000">V42</span> variable (numeric)
|      **[B.15]** <span style="color: #FF0000">V43</span> variable (numeric)
|      **[B.16]** <span style="color: #FF0000">V45</span> variable (numeric)
|      **[B.17]** <span style="color: #FF0000">V48</span> variable (numeric)
|
| **[C]** No low variance noted for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness noted for any variable with Skewness>3 or Skewness<(-3).
|
```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- Sonar_Train

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Index=c(1:length(names(DQA))),
  Column.Name= names(DQA), 
  Column.Type=sapply(DQA, function(x) class(x)), 
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all predictors
##################################
DQA.Predictors <- DQA[,!names(DQA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DQA.Predictors.Numeric <- DQA.Predictors[,sapply(DQA.Predictors, is.numeric)]

if (length(names(DQA.Predictors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Numeric))),
               " numeric predictor variable(s)."))
} else {
  print("There are no numeric predictor variables.")
}

##################################
# Listing all factor predictors
##################################
DQA.Predictors.Factor <- DQA.Predictors[,sapply(DQA.Predictors, is.factor)]

if (length(names(DQA.Predictors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Predictors.Factor))),
               " factor predictor variable(s)."))
} else {
  print("There are no factor predictor variables.")
}

##################################
# Formulating a data quality assessment summary for factor predictors
##################################
if (length(names(DQA.Predictors.Factor))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Factor), 
  Column.Type=sapply(DQA.Predictors.Factor, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Predictors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )
  
} 

##################################
# Formulating a data quality assessment summary for numeric predictors
##################################
if (length(names(DQA.Predictors.Numeric))>0) {
  
  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }
  
  (DQA.Predictors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Predictors.Numeric), 
  Column.Type=sapply(DQA.Predictors.Numeric, function(x) class(x)), 
  Unique.Count=sapply(DQA.Predictors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Predictors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Predictors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Predictors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Predictors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Predictors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Predictors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Predictors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Predictors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Predictors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Predictors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Predictors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )  
  
}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance predictors
##################################
if (length(names(DQA.Predictors.Factor))==0) {
  print("No factor predictors noted.")
} else if (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Factor.Summary[as.numeric(as.character(DQA.Predictors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric predictors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric predictors due to low unique count ratio noted.")
}

##################################
# Checking for skewed predictors
##################################
if (length(names(DQA.Predictors.Numeric))==0) {
  print("No numeric predictors noted.")
} else if (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Predictors.Numeric.Summary[as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Predictors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric predictors noted.")
}

```
##  1.3 Data Preprocessing

###  1.3.1 Outlier
|
| Outlier data assessment:
|
| **[A]** Outliers noted for 39 variables  with the numeric data visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile). Outlier treatment for numerical stability remains optional depending on potential model requirements for the subsequent steps.
|      **[A.1]** <span style="color: #FF0000">V1</span> variable (5 outliers detected)
|      **[A.2]** <span style="color: #FF0000">V2</span> variable (6 outliers detected)
|      **[A.3]** <span style="color: #FF0000">V3</span> variable (6 outliers detected)
|      **[A.4]** <span style="color: #FF0000">V4</span> variable (5 outliers detected)
|      **[A.5]** <span style="color: #FF0000">V5</span> variable (2 outliers detected)
|      **[A.6]** <span style="color: #FF0000">V6</span> variable (4 outliers detected)
|      **[A.7]** <span style="color: #FF0000">V7</span> variable (2 outliers detected)
|      **[A.8]** <span style="color: #FF0000">V8</span> variable (6 outliers detected)
|      **[A.9]** <span style="color: #FF0000">V9</span> variable (4 outliers detected)
|      **[A.10]** <span style="color: #FF0000">V10</span> variable (4 outliers detected)
|      **[A.11]** <span style="color: #FF0000">V11</span> variable (2 outliers detected)
|      **[A.13]** <span style="color: #FF0000">V13</span> variable (1 outlier detected)
|      **[A.14]** <span style="color: #FF0000">V14</span> variable (2 outliers detected)
|      **[A.15]** <span style="color: #FF0000">V15</span> variable (2 outliers detected)
|      **[A.16]** <span style="color: #FF0000">V24</span> variable (1 outlier detected)
|      **[A.16]** <span style="color: #FF0000">V25</span> variable (3 outliers detected)
|      **[A.17]** <span style="color: #FF0000">V38</span> variable (5 outliers detected)
|      **[A.18]** <span style="color: #FF0000">V39</span> variable (1 outlier detected)
|      **[A.19]** <span style="color: #FF0000">V40</span> variable (1 outliers detected)
|      **[A.20]** <span style="color: #FF0000">V41</span> variable (1 outlier detected)
|      **[A.21]** <span style="color: #FF0000">V42</span> variable (3 outliers detected)
|      **[A.22]** <span style="color: #FF0000">V43</span> variable (1 outliers detected)
|      **[A.23]** <span style="color: #FF0000">V44</span> variable (2 outliers detected)
|      **[A.25]** <span style="color: #FF0000">V46</span> variable (8 outliers detected)
|      **[A.26]** <span style="color: #FF0000">V47</span> variable (6 outliers detected)
|      **[A.27]** <span style="color: #FF0000">V48</span> variable (6 outliers detected)
|      **[A.28]** <span style="color: #FF0000">V49</span> variable (1 outliers detected)
|      **[A.29]** <span style="color: #FF0000">V50</span> variable (4 outliers detected)
|      **[A.30]** <span style="color: #FF0000">V51</span> variable (4 outliers detected)
|      **[A.31]** <span style="color: #FF0000">V52</span> variable (5 outliers detected)
|      **[A.32]** <span style="color: #FF0000">V53</span> variable (2 outliers detected)
|      **[A.33]** <span style="color: #FF0000">V54</span> variable (4 outliers detected)
|      **[A.34]** <span style="color: #FF0000">V55</span> variable (3 outliers detected)
|      **[A.35]** <span style="color: #FF0000">V56</span> variable (2 outliers detected)
|      **[A.36]** <span style="color: #FF0000">V57</span> variable (3 outliers detected)
|      **[A.37]** <span style="color: #FF0000">V58</span> variable (6 outliers detected)
|      **[A.38]** <span style="color: #FF0000">V59</span> variable (6 outliers detected)
|      **[A.39]** <span style="color: #FF0000">V60</span> variable (3 outliers detected)
|
```{r section_1.3.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=3}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying outliers for the numeric predictors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Predictors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Predictors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Predictors.Numeric[,i] %in% c(Outliers))
  boxplot(DPA.Predictors.Numeric[,i], 
          ylab = names(DPA.Predictors.Numeric)[i], 
          main = names(DPA.Predictors.Numeric)[i],
          horizontal=TRUE)
  mtext(paste0(OutlierCount, " Outlier(s) Detected"))
}

OutlierCountSummary <- as.data.frame(cbind(names(DPA.Predictors.Numeric),(OutlierCountList)))
names(OutlierCountSummary) <- c("NumericPredictors","OutlierCount")
OutlierCountSummary$OutlierCount <- as.numeric(as.character(OutlierCountSummary$OutlierCount))
NumericPredictorWithOutlierCount <- nrow(OutlierCountSummary[OutlierCountSummary$OutlierCount>0,])
print(paste0(NumericPredictorWithOutlierCount, " numeric variable(s) were noted with outlier(s)." ))

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Predictors.Numeric))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric)

```
###  1.3.2 Zero and Near-Zero Variance
|
| Zero and near-zero variance data assessment:
|
| **[A]** Low variance noted for 17 variables from the previous data quality assessment using a lower threshold.
|
| **[B]** No low variance noted for any variables using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 95/5,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){

  print("No low variance predictors noted.")

} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))

  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))

  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))

  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }

  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

  ##################################
  # Filtering out columns with low variance
  #################################
  DPA_ExcludedLowVariance <- DPA[,!names(DPA) %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,])]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLowVariance_Skimmed <- skim(DPA_ExcludedLowVariance))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedLowVariance)
  
} 

```
###  1.3.3 Collinearity
|
| High collinearity data assessment:
|
| **[A]** No high correlation > 95% were noted for any variable as confirmed using the preprocessing summaries from the <mark style="background-color: #CCECFF">**caret**</mark> and <mark style="background-color: #CCECFF">**lares**</mark> packages.
|
```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Visualizing pairwise correlation between predictors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Predictors.Numeric,
                       method = "pearson",
                       conf.level = .95)

corrplot(cor(DPA.Predictors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"), 
         method = "circle",
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.cex = 0.75,
         tl.srt = 90, 
         sig.level = 0.05, 
         p.mat = DPA_CorrelationTest$p,
         insig = "blank")



##################################
# Identifying the highly correlated variables
##################################
DPA_Correlation <-  cor(DPA.Predictors.Numeric, 
                        method = "pearson",
                        use="pairwise.complete.obs")
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)]) > 0.95))

if (DPA_HighlyCorrelatedCount == 0) {
  
  print("No highly correlated predictors noted.")
  
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.95."))

  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Predictors.Numeric,
  max_pvalue = 0.05,
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))

}


if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.95)

  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))

  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }

  ##################################
  # Filtering out columns with high correlation
  #################################
  DPA_ExcludedHighCorrelation <- DPA[,-DPA_HighlyCorrelated]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedHighCorrelation_Skimmed <- skim(DPA_ExcludedHighCorrelation))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedHighCorrelation)

}

```
###  1.3.4 Linear Dependencies
|
| Linear dependency data assessment:
|
| **[A]** No linear dependencies noted for any subset of variables using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))

  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }

}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)

  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)

  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

  ##################################
  # Filtering out columns with linear dependency
  #################################
  DPA_ExcludedLinearlyDependent <- DPA[,-DPA_LinearlyDependent$remove]

  ##################################
  # Gathering descriptive statistics
  ##################################
  (DPA_ExcludedLinearlyDependent_Skimmed <- skim(DPA_ExcludedLinearlyDependent))
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA_ExcludedLinearlyDependent)

} else {
  
  ###################################
  # Verifying the data dimensions
  ###################################
  dim(DPA)
  
}

```

###  1.3.5 Shape Transformation
|
| Data transformation assessment:
|
| **[A]** A number of numeric variables in the dataset were observed to be right-skewed which required shape transformation for data distribution stability. Considering that all numeric variables were strictly positive values, the <span style="color: #0000FF">BoxCox</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was used to transform their distributional shapes.
|
```{r section_1.3.5, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

##################################
# Gathering descriptive statistics
##################################
(DPA_BoxCoxTransformedSkimmed <- skim(DPA_BoxCoxTransformed))

###################################
# Verifying the data dimensions
###################################
dim(DPA_BoxCoxTransformed)

```

###  1.3.6 Centering and Scaling
|
| Centering and scaling data assessment:
|
| **[A]** To maintain numerical stability during modelling, centering and scaling transformations were applied on the transformed numeric variables. The <span style="color: #0000FF">center</span> method from the <mark style="background-color: #CCECFF">**caret**</mark> package was implemented which subtracts the average value of a numeric variable to all the values. As a result of centering, the variables had zero mean values. In addition, the <span style="color: #0000FF">scale</span> method, also from the <mark style="background-color: #CCECFF">**caret**</mark> package, was applied which performs a center transformation with each value of the variable divided by its standard deviation. Scaling the data coerced the values to have a common standard deviation of one.
|
```{r section_1.3.6, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- Sonar_Train

##################################
# Listing all predictors
##################################
DPA.Predictors <- DPA[,!names(DPA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
DPA.Predictors.Numeric <- DPA.Predictors[,sapply(DPA.Predictors, is.numeric)]

##################################
# Applying a Box-Cox transformation
##################################
DPA_BoxCox <- preProcess(DPA.Predictors.Numeric, method = c("BoxCox"))
DPA_BoxCoxTransformed <- predict(DPA_BoxCox, DPA.Predictors.Numeric)

##################################
# Applying a center and scale data transformation
##################################
DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaled <- preProcess(DPA_BoxCoxTransformed, method = c("center","scale"))
DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed <- predict(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaled, DPA_BoxCoxTransformed)

##################################
# Gathering descriptive statistics
##################################
(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformedSkimmed <- skim(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed))

###################################
# Verifying the data dimensions
###################################
dim(DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed)

```

###  1.3.7 Pre-Processed Dataset
|
| Preliminary dataset assessment:
|
| **[A]** 136 rows (observations)
|      **[A.1]** Train Set = 96 observations with class ratio of 80:20
|      **[A.2]** Test Set = 40 observations with class ratio of 80:20
| 
| **[B]** 61 columns (variables)
|      **[B.1]** 1/61 response = <span style="color: #FF0000">Class</span> variable (factor)
|             **[B.1.1]** Levels = <span style="color: #FF0000">Class=R</span> < <span style="color: #FF0000">Class=M</span>
|      **[B.2]** 60/61 predictors = All remaining variables (60/60 numeric)
|
| **[C]** Pre-processing actions applied:
|      **[C.1]** Centering, scaling and shape transformation applied to improve data quality
|      **[C.2]** No outlier treatment applied since the high values noted were contextually valid and sensible 
|      **[C.3]** No predictors removed due to zero or near-zero variance 
|      **[C.4]** No predictors removed due to high correlation
|      **[C.5]** No predictors removed due to linear dependencies
|
```{r section_1.3.7, warning=FALSE, message=FALSE}
##################################
# Creating the pre-modelling
# train set
##################################
Class <- DPA$Class 
PMA.Predictors.Numeric  <- DPA.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed
PMA_BoxCoxTransformed_CenteredScaledTransformed <- cbind(Class,PMA.Predictors.Numeric)
PMA_PreModelling_Train <- PMA_BoxCoxTransformed_CenteredScaledTransformed

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Train_Skimmed <- skim(PMA_PreModelling_Train))

###################################
# Verifying the data dimensions
# for the train set
###################################
dim(PMA_PreModelling_Train)

##################################
# Formulating the test set
##################################
DPA_Test <- Sonar_Test
DPA_Test.Predictors <- DPA_Test[,!names(DPA_Test) %in% c("Class")]
DPA_Test.Predictors.Numeric <- DPA_Test.Predictors[,sapply(DPA_Test.Predictors, is.numeric)]
DPA_Test_BoxCox <- preProcess(DPA_Test.Predictors.Numeric, method = c("BoxCox"))
DPA_Test_BoxCoxTransformed <- predict(DPA_Test_BoxCox, DPA_Test.Predictors.Numeric)
DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaled <- preProcess(DPA_Test_BoxCoxTransformed, method = c("center","scale"))
DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed <- predict(DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaled, DPA_Test_BoxCoxTransformed)

##################################
# Creating the pre-modelling
# test set
##################################
Class <- DPA_Test$Class 
PMA_Test.Predictors.Numeric  <- DPA_Test.Predictors.Numeric_BoxCoxTransformed_CenteredScaledTransformed
PMA_Test_BoxCoxTransformed_CenteredScaledTransformed <- cbind(Class,PMA_Test.Predictors.Numeric)
PMA_PreModelling_Test <- PMA_Test_BoxCoxTransformed_CenteredScaledTransformed

##################################
# Gathering descriptive statistics
##################################
(PMA_PreModelling_Test_Skimmed <- skim(PMA_PreModelling_Test))

###################################
# Verifying the data dimensions
# for the test set
###################################
dim(PMA_PreModelling_Test)

```
## 1.4 Data Exploration
|
| Exploratory data analysis:
|
| **[A]** Numeric variables which demonstrated a differential relationships with the <span style="color: #FF0000">Class</span> response variable include:
|      **[A.1]** <span style="color: #FF0000">V1</span> variable (numeric)
|      **[A.2]** <span style="color: #FF0000">V4</span> variable (numeric)
|      **[A.3]** <span style="color: #FF0000">V9</span> variable (numeric)
|      **[A.4]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[A.5]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[A.6]** <span style="color: #FF0000">V12</span> variable (numeric)
|      **[A.7]** <span style="color: #FF0000">V13</span> variable (numeric)
|      **[A.8]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[A.9]** <span style="color: #FF0000">V45</span> variable (numeric)
|      **[A.10]** <span style="color: #FF0000">V48</span> variable (numeric)
|      **[A.11]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[A.12]** <span style="color: #FF0000">V50</span> variable (numeric)
|      **[A.13]** <span style="color: #FF0000">V51</span> variable (numeric)
|
```{r section_1.4, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
EDA <- PMA_PreModelling_Train

##################################
# Listing all predictors
##################################
EDA.Predictors <- EDA[,!names(EDA) %in% c("Class")]

##################################
# Listing all numeric predictors
##################################
EDA.Predictors.Numeric <- EDA.Predictors[,sapply(EDA.Predictors, is.numeric)]
ncol(EDA.Predictors.Numeric)
names(EDA.Predictors.Numeric)

##################################
# Formulating the box plots
##################################
featurePlot(x = EDA.Predictors.Numeric, 
            y = EDA$Class,
            plot = "box",
            scales = list(x = list(relation="free", rot = 90), 
                          y = list(relation="free")),
            adjust = 1.5, 
            pch = "|")

```

## 1.5 Cost-Sensitive Learning Applied for Class Imbalance

###  1.5.1 Support Vector Machine - Radial Basis Function Kernel (SVM_R)
|
| [Support Vector Machine](http://www.cs.cmu.edu/~pakyan/compbio/references/Drucker_NIPS_1996.pdf) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. This version of the model applied uniform case weights between the classes of the response variable (<span style="color: #FF0000">Class=M</span>:1, <span style="color: #FF0000">Class=R</span>:1). 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma held constant at a value of 0.00873
|      **[B.2]** <span style="color: #FF0000">C</span> = cost made to vary across a range of 2^(-6) to 2^(1)
|
| **[C]** Specificity was particularly used as the metric for assessment to compare the effect of cost-sensitive learning to the minority class.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves sigma=0.00873 and C=2.00000
|      **[D.2]** Specificity = 0.60000
|
| **[E]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** Specificity = 0.14286
|
```{r section_1.5.1, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating a function for 
# a customized summary metrics
##################################
fourMetricSummary <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sensitivity", "Specificity")
  out
}

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
KFold_Control <- trainControl(method = "cv", 
                                   classProbs = FALSE,
                                   summaryFunction = fourMetricSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
##################################
# Pre-computing the sigma parameter
##################################
set.seed(12345678)
sigma <- sigest(Class~.,
                data=PMA_PreModelling_Train,
                frac=0.75)
names(sigma)=NULL
SVM_R_Grid <- data.frame(sigma=sigma[2], C=2^seq(-6,1,length=15))

##################################
# Running the support vector machine (radial basis function kernel) model
# by setting the caret method to 'svmRadial'
##################################
set.seed(12345678)
SVM_R_Tune <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                 y = PMA_PreModelling_Train$Class,
                 method = "svmRadial",
                 tuneGrid = SVM_R_Grid,
                 metric = "Specificity",                 
                 preProc = c("center", "scale"),
                 trControl = KFold_Control,
                 class.weights = c(M=1,R=1))

##################################
# Reporting the cross-validation results
# for the train set
##################################
SVM_R_Tune

SVM_R_Tune$finalModel

SVM_R_Tune$results

(SVM_R_Train_Specificity <- SVM_R_Tune$results[SVM_R_Tune$results$C==SVM_R_Tune$bestTune$C,
                              c("Specificity")])

SVM_R_Train <- data.frame(SVM_R_Observed = PMA_PreModelling_Train$Class,
                      SVM_R_Predicted = predict(SVM_R_Tune, 
                      PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                      type = "raw"))

(SVM_R_Train_ConfusionMatrix <- confusionMatrix(data = SVM_R_Train$SVM_R_Predicted,
                                                  reference = SVM_R_Train$SVM_R_Observed))


##################################
# Identifying and plotting the
# best model predictors
##################################
# model does not support variable importance measurement

##################################
# Independently evaluating the model
# on the test set
##################################
SVM_R_Test <- data.frame(SVM_R_Observed = PMA_PreModelling_Test$Class,
                      SVM_R_Predicted = predict(SVM_R_Tune, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

SVM_R_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(SVM_R_Test_Specificity <- Specificity(y_pred = SVM_R_Test$SVM_R_Predicted,
                                       y_true = SVM_R_Test$SVM_R_Observed))

(SVM_R_Test_ConfusionMatrix <- confusionMatrix(data = SVM_R_Test$SVM_R_Predicted,
                                               reference = SVM_R_Test$SVM_R_Observed))

```

###  1.5.2 Class-Weighted Support Vector Machine - Radial Basis Function Kernel (CW_SVM_R)
|
| [Support Vector Machine](http://www.cs.cmu.edu/~pakyan/compbio/references/Drucker_NIPS_1996.pdf) plots each observation in an N-dimensional space corresponding to the number of features in the data set and finds a hyperplane that maximally separates the different classes by a maximally large margin (which is defined as the distance between the hyperplane and the closest data points from each class). The algorithm applies kernel transformation by mapping non-linearly separable data using the similarities between the points in a high-dimensional feature space for improved discrimination.
|
| **[A]** The support vector machine (radial basis function kernel) model from the  <mark style="background-color: #CCECFF">**kernlab**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. This version of the model applied non-uniform case weights between the classes of the response variable (<span style="color: #FF0000">Class=M</span>:1, <span style="color: #FF0000">Class=R</span>:4). 
|
| **[B]** The model contains 2 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">sigma</span> = sigma held constant at a value of 0.00873
|      **[B.2]** <span style="color: #FF0000">C</span> = cost made to vary across a range of 2^(-6) to 2^(1)
|
| **[C]** Specificity was particularly used as the metric for assessment to compare the effect of cost-sensitive learning to the minority class.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves sigma=0.00873 and C=0.35355
|      **[D.2]** Specificity = 0.65000
|
| **[E]** The model does not allow for ranking of predictors in terms of variable importance.
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** Specificity = 0.28571
|
```{r section_1.5.2, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating a function for 
# a customized summary metrics
##################################
fourMetricSummary <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sensitivity", "Specificity")
  out
}

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
KFold_Control <- trainControl(method = "cv", 
                                   classProbs = FALSE,
                                   summaryFunction = fourMetricSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
##################################
# Pre-computing the sigma parameter
##################################
set.seed(12345678)
sigma <- sigest(Class~.,
                data=PMA_PreModelling_Train,
                frac=0.75)
names(sigma)=NULL
CW_SVM_R_Grid <- data.frame(sigma=sigma[2], C=2^seq(-6,1,length=15))

##################################
# Running the support vector machine (radial basis function kernel) model
# by setting the caret method to 'svmRadial'
##################################
set.seed(12345678)
CW_SVM_R_Tune <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                 y = PMA_PreModelling_Train$Class,
                 method = "svmRadial",
                 tuneGrid = CW_SVM_R_Grid,
                 metric = "Specificity",                 
                 preProc = c("center", "scale"),
                 trControl = KFold_Control,
                 class.weights = c(M=1,R=4))

##################################
# Reporting the cross-validation results
# for the train set
##################################
CW_SVM_R_Tune

CW_SVM_R_Tune$finalModel

CW_SVM_R_Tune$results

(CW_SVM_R_Train_Specificity <- CW_SVM_R_Tune$results[CW_SVM_R_Tune$results$C==CW_SVM_R_Tune$bestTune$C,
                              c("Specificity")])

CW_SVM_R_Train <- data.frame(CW_SVM_R_Observed = PMA_PreModelling_Train$Class,
                      CW_SVM_R_Predicted = predict(CW_SVM_R_Tune, 
                      PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                      type = "raw"))

(CW_SVM_R_Train_ConfusionMatrix <- confusionMatrix(data = CW_SVM_R_Train$CW_SVM_R_Predicted,
                                                  reference = CW_SVM_R_Train$CW_SVM_R_Observed))


##################################
# Identifying and plotting the
# best model predictors
##################################
# model does not support variable importance measurement

##################################
# Independently evaluating the model
# on the test set
##################################
CW_SVM_R_Test <- data.frame(CW_SVM_R_Observed = PMA_PreModelling_Test$Class,
                      CW_SVM_R_Predicted = predict(CW_SVM_R_Tune, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

CW_SVM_R_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(CW_SVM_R_Test_Specificity <- Specificity(y_pred = CW_SVM_R_Test$CW_SVM_R_Predicted,
                                          y_true = CW_SVM_R_Test$CW_SVM_R_Observed))

(CW_SVM_R_Test_ConfusionMatrix <- confusionMatrix(data = CW_SVM_R_Test$CW_SVM_R_Predicted,
                                                 reference = CW_SVM_R_Test$CW_SVM_R_Observed))

```

###  1.5.3 Classification and Regression Trees (CART)
|
| [Classification and Regression Trees](https://www.semanticscholar.org/paper/Classification-and-Regression-Trees-Breiman-Friedman/8017699564136f93af21575810d557dba1ee6fc6) constructs binary trees for both both nominal and continuous input attributes using Gini Index as its splitting criteria. The algorithm handles missing values by surrogating tests to approximate outcomes. In the pruning phase, CART uses pre-pruning technique called Cost-Complexity pruning to remove redundant branches from the decision tree to improve the accuracy. In the first stage, a sequence of increasingly smaller trees are built on the training data. In the second stage, one of these tree is chosen as the pruned tree, based on its classification accuracy on a pruning set, adopting a cross-validated method in its pruning technique.
|
| **[A]** The classification and regression trees model from the  <mark style="background-color: #CCECFF">**rpart**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. This version of the model applied uniform case weights between the classes of the response variable (<span style="color: #FF0000">Class=M</span>:1, <span style="color: #FF0000">Class=R</span>:1). 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">cp</span> = complexity parameter threshold made to vary across a range of values equal to 0.0001 to 0.0200
|
| **[C]** Specificity was particularly used as the metric for assessment to compare the effect of cost-sensitive learning to the minority class.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves cp=0.0200
|      **[D.2]** Specificity = 0.45000
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V22</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V10</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** Specificity = 0.28571
|
```{r section_1.5.3, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating a function for 
# a customized summary metrics
##################################
fourMetricSummary <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sensitivity", "Specificity")
  out
}

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
KFold_Control <- trainControl(method = "cv", 
                                   classProbs = FALSE,
                                   summaryFunction = fourMetricSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
CART_Grid = data.frame(cp = c(0.0001, 0.0005, 0.001, 0.005, 0.010, 0.015, 0.020))

##################################
# Formulating the cost matrix
##################################
CART_CostMatrix <- matrix(c(0,1,1,0), ncol=2)
rownames(CART_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
colnames(CART_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
CART_CostMatrix

##################################
# Running the classification and regression trees model
# by setting the caret method to 'rpart'
##################################
set.seed(12345678)
CART_Tune <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                 y = PMA_PreModelling_Train$Class,
                 method = "rpart",
                 tuneGrid = CART_Grid,
                 metric = "Specificity",                 
                 trControl = KFold_Control,
                 parms = list(loss=CART_CostMatrix))

##################################
# Reporting the cross-validation results
# for the train set
##################################
CART_Tune

CART_Tune$finalModel

CART_Tune$results

(CART_Train_Specificity <- CART_Tune$results[CART_Tune$results$cp==CART_Tune$bestTune$cp,
                              c("Specificity")])

CART_Train <- data.frame(CART_Observed = PMA_PreModelling_Train$Class,
                      CART_Predicted = predict(CART_Tune, 
                      PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                      type = "raw"))

(CART_Train_ConfusionMatrix <- confusionMatrix(data = CART_Train$CART_Predicted,
                                                  reference = CART_Train$CART_Observed))


##################################
# Identifying and plotting the
# best model predictors
##################################
CART_VarImp <- varImp(CART_Tune, scale = TRUE)
plot(CART_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Classification and Regression Trees",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
CART_Test <- data.frame(CART_Observed = PMA_PreModelling_Test$Class,
                      CART_Predicted = predict(CART_Tune, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

CART_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(CART_Test_Specificity <- Specificity(y_pred = CART_Test$CART_Predicted,
                                       y_true = CART_Test$CART_Observed))

(CART_Test_ConfusionMatrix <- confusionMatrix(data = CART_Test$CART_Predicted,
                                               reference = CART_Test$CART_Observed))

```

###  1.5.4 Cost-Sensitive Classification and Regression Trees (CS_CART)
|
| [Classification and Regression Trees](https://www.semanticscholar.org/paper/Classification-and-Regression-Trees-Breiman-Friedman/8017699564136f93af21575810d557dba1ee6fc6) constructs binary trees for both both nominal and continuous input attributes using Gini Index as its splitting criteria. The algorithm handles missing values by surrogating tests to approximate outcomes. In the pruning phase, CART uses pre-pruning technique called Cost-Complexity pruning to remove redundant branches from the decision tree to improve the accuracy. In the first stage, a sequence of increasingly smaller trees are built on the training data. In the second stage, one of these tree is chosen as the pruned tree, based on its classification accuracy on a pruning set, adopting a cross-validated method in its pruning technique.
|
| **[A]** The classification and regression trees model from the  <mark style="background-color: #CCECFF">**rpart**</mark> package was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. This version of the model applied non-uniform case weights between the classes of the response variable (<span style="color: #FF0000">Class=M</span>:1, <span style="color: #FF0000">Class=R</span>:4). 
|
| **[B]** The model contains 1 hyperparameter:
|      **[B.1]** <span style="color: #FF0000">cp</span> = complexity parameter threshold made to vary across a range of values equal to 0.0001 to 0.0200
|
| **[C]** Specificity was particularly used as the metric for assessment to compare the effect of cost-sensitive learning to the minority class.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves cp=0.0200
|      **[D.2]** Specificity = 0.30000
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V21</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V10</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V35</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V48</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** Specificity = 0.71428
|
```{r section_1.5.4, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating a function for 
# a customized summary metrics
##################################
fourMetricSummary <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sensitivity", "Specificity")
  out
}

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
KFold_Control <- trainControl(method = "cv", 
                                   classProbs = FALSE,
                                   summaryFunction = fourMetricSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
CS_CART_Grid = data.frame(cp = c(0.0001, 0.0005, 0.0010, 0.0050, 0.0100, 0.0150, 0.0200))

##################################
# Formulating the cost matrix
##################################
CS_CART_CostMatrix <- matrix(c(0,4,1,0), ncol=2)
rownames(CS_CART_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
colnames(CS_CART_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
CS_CART_CostMatrix

##################################
# Running the classification and regression trees model
# by setting the caret method to 'rpart'
##################################
set.seed(12345678)
CS_CART_Tune <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                 y = PMA_PreModelling_Train$Class,
                 method = "rpart",
                 tuneGrid = CS_CART_Grid,
                 metric = "Specificity",                 
                 trControl = KFold_Control,
                 parms = list(loss=CS_CART_CostMatrix))

##################################
# Reporting the cross-validation results
# for the train set
##################################
CS_CART_Tune

CS_CART_Tune$finalModel

CS_CART_Tune$results

(CS_CART_Train_Specificity <- CS_CART_Tune$results[CS_CART_Tune$results$cp==CS_CART_Tune$bestTune$cp,
                              c("Specificity")])

CS_CART_Train <- data.frame(CS_CART_Observed = PMA_PreModelling_Train$Class,
                      CS_CART_Predicted = predict(CS_CART_Tune, 
                      PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                      type = "raw"))

(CS_CART_Train_ConfusionMatrix <- confusionMatrix(data = CS_CART_Train$CS_CART_Predicted,
                                                  reference = CS_CART_Train$CS_CART_Observed))


##################################
# Identifying and plotting the
# best model predictors
##################################
CS_CART_VarImp <- varImp(CS_CART_Tune, scale = TRUE)
plot(CS_CART_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Cost-Sensitive Classification and Regression Trees",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
CS_CART_Test <- data.frame(CS_CART_Observed = PMA_PreModelling_Test$Class,
                      CS_CART_Predicted = predict(CS_CART_Tune, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

CS_CART_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(CS_CART_Test_Specificity <- Specificity(y_pred = CS_CART_Test$CS_CART_Predicted,
                                       y_true = CS_CART_Test$CS_CART_Observed))

(CS_CART_Test_ConfusionMatrix <- confusionMatrix(data = CS_CART_Test$CS_CART_Predicted,
                                               reference = CS_CART_Test$CS_CART_Observed))

```

###  1.5.5 C5.0 Decision Trees (C50)
|
| [C5.0 Decision Trees](https://link.springer.com/article/10.1007/BF00993309) generates multi-branch trees in a situation where one or more nominal inputs are given, using an information-based criterion (Entropy and Information Gain) as an attribute selection measure to build decision trees. For overfitting avoidance, the algorithm applies a pessimistic pruning approach called Rule-post pruning, to remove unreliable branches from the decision tree to reduce the size of the tree without any loss of its predictive accuracy. The Rule-post pruning starts off by converting a decision tree to an equivalent set of rules, then based on statistical confidence estimations for error rate it evaluates the rules with the aim of simplifying them without affecting the accuracy, adopting the Binomial Confidence Limit method. In a case of handling missing values, the algorithm allows to whether estimate missing values as a function of other attributes or apportions the case probabilistically among the results.
|
| **[A]** The C5.0 decision trees model from the  <mark style="background-color: #CCECFF">**C50**</mark> and <mark style="background-color: #CCECFF">**plyr**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. This version of the model applied uniform case weights between the classes of the response variable (<span style="color: #FF0000">Class=M</span>:1, <span style="color: #FF0000">Class=R</span>:1). 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">trials</span> = number of boosting iterations made to vary across a range of values equal to 1 to 100
|      **[B.2]** <span style="color: #FF0000">model</span> = model type made to vary across a range of levels equal to TREE and RULES
|      **[B.3]** <span style="color: #FF0000">winnow</span> = winnow made to vary across a range of levels equal to TRUE and FALSE
|
| **[C]** Specificity was particularly used as the metric for assessment to compare the effect of cost-sensitive learning to the minority class.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves trials=1, model=TREE and winnow=FALSE
|      **[D.2]** Specificity = 0.55000
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V37</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V17</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V20</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** Specificity = 0.14286
|
```{r section_1.5.5, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating a function for 
# a customized summary metrics
##################################
fourMetricSummary <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sensitivity", "Specificity")
  out
}

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
KFold_Control <- trainControl(method = "cv", 
                                   classProbs = FALSE,
                                   summaryFunction = fourMetricSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
C50_Grid = expand.grid(trials = c(1:9, (1:10)*10),
                       model = c("tree", "rules"),
                       winnow = c(TRUE, FALSE))

##################################
# Formulating the cost matrix
##################################
C50_CostMatrix <- matrix(c(0,1,1,0), ncol=2)
rownames(C50_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
colnames(C50_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
C50_CostMatrix

##################################
# Running the C5.0 decision trees model
# by setting the caret method to 'C5.0'
##################################
set.seed(12345678)
C50_Tune <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                 y = PMA_PreModelling_Train$Class,
                 method = "C5.0",
                 tuneGrid = C50_Grid,
                 metric = "Specificity",                 
                 trControl = KFold_Control,
                 parms = list(loss=C50_CostMatrix))

##################################
# Reporting the cross-validation results
# for the train set
##################################
C50_Tune

C50_Tune$finalModel

C50_Tune$results

(C50_Train_Specificity <- C50_Tune$results[C50_Tune$results$trials==C50_Tune$bestTune$trials &
                                             C50_Tune$results$model==C50_Tune$bestTune$model &
                                             C50_Tune$results$winnow==C50_Tune$bestTune$winnow,
                              c("Specificity")])

C50_Train <- data.frame(C50_Observed = PMA_PreModelling_Train$Class,
                      C50_Predicted = predict(C50_Tune, 
                      PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                      type = "raw"))

(C50_Train_ConfusionMatrix <- confusionMatrix(data = C50_Train$C50_Predicted,
                                                  reference = C50_Train$C50_Observed))


##################################
# Identifying and plotting the
# best model predictors
##################################
C50_VarImp <- varImp(C50_Tune, scale = TRUE)
plot(C50_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : C5.0 Decision Trees",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
C50_Test <- data.frame(C50_Observed = PMA_PreModelling_Test$Class,
                      C50_Predicted = predict(C50_Tune, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

C50_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(C50_Test_Specificity <- Specificity(y_pred = C50_Test$C50_Predicted,
                                       y_true = C50_Test$C50_Observed))

(C50_Test_ConfusionMatrix <- confusionMatrix(data = C50_Test$C50_Predicted,
                                               reference = C50_Test$C50_Observed))

```

###  1.5.6 Cost-Sensitive C5.0 Decision Trees (CS_C50)
|
| [C5.0 Decision Trees](https://link.springer.com/article/10.1007/BF00993309) generates multi-branch trees in a situation where one or more nominal inputs are given, using an information-based criterion (Entropy and Information Gain) as an attribute selection measure to build decision trees. For overfitting avoidance, the algorithm applies a pessimistic pruning approach called Rule-post pruning, to remove unreliable branches from the decision tree to reduce the size of the tree without any loss of its predictive accuracy. The Rule-post pruning starts off by converting a decision tree to an equivalent set of rules, then based on statistical confidence estimations for error rate it evaluates the rules with the aim of simplifying them without affecting the accuracy, adopting the Binomial Confidence Limit method. In a case of handling missing values, the algorithm allows to whether estimate missing values as a function of other attributes or apportions the case probabilistically among the results.
|
| **[A]** The C5.0 decision trees model from the  <mark style="background-color: #CCECFF">**C50**</mark> and <mark style="background-color: #CCECFF">**plyr**</mark> packages was implemented through the <mark style="background-color: #CCECFF">**caret**</mark> package. This version of the model applied non-uniform case weights between the classes of the response variable (<span style="color: #FF0000">Class=M</span>:1, <span style="color: #FF0000">Class=R</span>:4). 
|
| **[B]** The model contains 3 hyperparameters:
|      **[B.1]** <span style="color: #FF0000">trials</span> = number of boosting iterations made to vary across a range of values equal to 1 to 100
|      **[B.2]** <span style="color: #FF0000">model</span> = model type made to vary across a range of levels equal to TREE and RULES
|      **[B.3]** <span style="color: #FF0000">winnow</span> = winnow made to vary across a range of levels equal to TRUE and FALSE
|
| **[C]** Specificity was particularly used as the metric for assessment to compare the effect of cost-sensitive learning to the minority class.
|
| **[D]** The cross-validated model performance of the final model is summarized as follows:
|      **[D.1]** Final model configuration involves trials=1, model=TREE and winnow=FALSE
|      **[D.2]** Specificity = 0.55000
|
| **[E]** The model allows for ranking of predictors in terms of variable importance. The top-performing predictors in the model are as follows:
|      **[E.1]** <span style="color: #FF0000">V11</span> variable (numeric)
|      **[E.2]** <span style="color: #FF0000">V49</span> variable (numeric)
|      **[E.3]** <span style="color: #FF0000">V37</span> variable (numeric)
|      **[E.4]** <span style="color: #FF0000">V17</span> variable (numeric)
|      **[E.5]** <span style="color: #FF0000">V20</span> variable (numeric)
|
| **[F]** The independent test model performance of the final model is summarized as follows:
|      **[F.1]** Specificity = 0.14286
|
```{r section_1.5.6, warning=FALSE, message=FALSE}
##################################
# Verifying the class distribution
# for the original data
##################################
table(PMA_PreModelling_Train$Class) 

##################################
# Creating a function for 
# a customized summary metrics
##################################
fourMetricSummary <- function (data, lev = levels(data$obs), model = NULL)
{

  accKapp <- postResample(data[, "pred"], data[, "obs"])
  out <- c(accKapp,
           sensitivity(data[, "pred"], data[, "obs"], lev[1]),
           specificity(data[, "pred"], data[, "obs"], lev[2]))
  names(out)[3:4] <- c("Sensitivity", "Specificity")
  out
}

##################################
# Creating consistent fold assignments 
# for the Repeated Cross Validation process
##################################
set.seed(12345678)
KFold_Control <- trainControl(method = "cv", 
                                   classProbs = FALSE,
                                   summaryFunction = fourMetricSummary)

##################################
# Setting the conditions
# for hyperparameter tuning
##################################
CS_C50_Grid = expand.grid(trials = c(1:9, (1:10)*10),
                       model = c("tree", "rules"),
                       winnow = c(TRUE, FALSE))

##################################
# Formulating the cost matrix
##################################
CS_C50_CostMatrix <- matrix(c(0,1,4,0), ncol=2)
rownames(CS_C50_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
colnames(CS_C50_CostMatrix) <- levels(PMA_PreModelling_Train$Class)
CS_C50_CostMatrix

##################################
# Running the C5.0 decision trees model
# by setting the caret method to 'C5.0'
##################################
set.seed(12345678)
CS_C50_Tune <- train(x = PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                 y = PMA_PreModelling_Train$Class,
                 method = "C5.0",
                 tuneGrid = CS_C50_Grid,
                 metric = "Specificity",                 
                 trControl = KFold_Control,
                 parms = list(loss=CS_C50_CostMatrix))

##################################
# Reporting the cross-validation results
# for the train set
##################################
CS_C50_Tune

CS_C50_Tune$finalModel

CS_C50_Tune$results

(CS_C50_Train_Specificity <- CS_C50_Tune$results[CS_C50_Tune$results$trials==CS_C50_Tune$bestTune$trials &
                                             CS_C50_Tune$results$model==CS_C50_Tune$bestTune$model &
                                             CS_C50_Tune$results$winnow==CS_C50_Tune$bestTune$winnow,
                              c("Specificity")])

CS_C50_Train <- data.frame(CS_C50_Observed = PMA_PreModelling_Train$Class,
                      CS_C50_Predicted = predict(CS_C50_Tune, 
                      PMA_PreModelling_Train[,!names(PMA_PreModelling_Train) %in% c("Class")],
                      type = "raw"))

(CS_C50_Train_ConfusionMatrix <- confusionMatrix(data = CS_C50_Train$CS_C50_Predicted,
                                                  reference = CS_C50_Train$CS_C50_Observed))


##################################
# Identifying and plotting the
# best model predictors
##################################
CS_C50_VarImp <- varImp(CS_C50_Tune, scale = TRUE)
plot(CS_C50_VarImp,
     top=25,
     scales=list(y=list(cex = .95)),
     main="Ranked Variable Importance : Cost-Sensitive C5.0 Decision Trees",
     xlab="Scaled Variable Importance Metrics",
     ylab="Predictors",
     cex=2,
     origin=0,
     alpha=0.45)

##################################
# Independently evaluating the model
# on the test set
##################################
CS_C50_Test <- data.frame(CS_C50_Observed = PMA_PreModelling_Test$Class,
                      CS_C50_Predicted = predict(CS_C50_Tune, 
                      PMA_PreModelling_Test[,!names(PMA_PreModelling_Test) %in% c("Class")],
                      type = "raw"))

CS_C50_Test

##################################
# Reporting the independent evaluation results
# for the test set
##################################
(CS_C50_Test_Specificity <- Specificity(y_pred = CS_C50_Test$CS_C50_Predicted,
                                       y_true = CS_C50_Test$CS_C50_Observed))

(CS_C50_Test_ConfusionMatrix <- confusionMatrix(data = CS_C50_Test$CS_C50_Predicted,
                                               reference = CS_C50_Test$CS_C50_Observed))

```
## 1.6 Evaluation Summary
|
| Model performance comparison:
|
| **[A]** The model which demonstrated the most consistent improvement in specificity metric after cost-sensitive training is as follows:
|      **[A.1]** SVM_R: Support Vector Machine - Radial Basis Function Kernel
|             **[A.1.1]** Test Specificity = 0.14286, Cross-Validation Specificity = 0.60000
|      **[A.2]** CW_SVM_R: Class-Weighted Support Vector Machine - Radial Basis Function Kernel ()
|             **[A.2.1]** Test Specificity = 0.28571, Cross-Validation Specificity = 0.65000
|
```{r section_1.7, warning=FALSE, message=FALSE}
##################################
# Consolidating all evaluation results
# for the train and test sets
# using the specificity metric
##################################
Model <- c('SVM_R','CW_SVM_R','CART','CS_CART','C50','CS_C50',
           'SVM_R','CW_SVM_R','CART','CS_CART','C50','CS_C50')

Set <- c(rep('Cross-Validation',6),rep('Test',6))

Specificity <- c(SVM_R_Train_Specificity,
                 CW_SVM_R_Train_Specificity,
                 CART_Train_Specificity,
                 CS_CART_Train_Specificity,
                 C50_Train_Specificity,
                 CS_C50_Train_Specificity,
                 SVM_R_Test_Specificity,
                 CW_SVM_R_Test_Specificity,
                 CART_Test_Specificity,
                 CS_CART_Test_Specificity,
                 C50_Test_Specificity,
                 CS_C50_Test_Specificity)

Specificity_Summary <- as.data.frame(cbind(Model,Set,Specificity))

Specificity_Summary$Specificity <- as.numeric(as.character(Specificity_Summary$Specificity))
Specificity_Summary$Set <- factor(Specificity_Summary$Set,
                                        levels = c("Cross-Validation",
                                                   "Test"))
Specificity_Summary$Model <- factor(Specificity_Summary$Model,
                                        levels =c('SVM_R',
                                                  'CW_SVM_R',
                                                  'CART',
                                                  'CS_CART',
                                                  'C50',
                                                  'CS_C50'))

print(Specificity_Summary, row.names=FALSE)

(Specificity_Plot <- dotplot(Model ~ Specificity,
                           data = Specificity_Summary,
                           groups = Set,
                           main = "Classification Model Performance Comparison",
                           ylab = "Model",
                           xlab = "Specificity",
                           auto.key = list(adj = 1),
                           type=c("p", "h"),       
                           origin = 0,
                           alpha = 0.45,
                           pch = 16,
                           cex = 2))

##################################
# Consolidating the resampling results
# for the candidate models
##################################
(COST_COMPARISON_RESAMPLING <- resamples(list(SVM_R = SVM_R_Tune,
                                              CW_SVM_R = SVM_R_Tune,
                                              CART = CART_Tune,
                                              CS_CART = CS_CART_Tune,
                                              C50 = C50_Tune,
                                              CS_C50 = CS_C50_Tune)))

summary(COST_COMPARISON_RESAMPLING)

##################################
# Exploring the resampling results
##################################
bwplot(COST_COMPARISON_RESAMPLING,
       main = "Model Resampling Performance Comparison (Range)",
       ylab = "Model",
       pch = 16,
       cex = 2,
       layout=c(4,1))

dotplot(COST_COMPARISON_RESAMPLING,
       main = "Model Resampling Performance Comparison (95% Confidence Interval)",
       ylab = "Model",
       pch = 16,
       cex = 2,        
       layout=c(4,1))

##################################
# Consolidating all models
##################################
(COST_COMPARISON_MODELS <- (list(SVM_R = SVM_R_Tune,
                                 CW_SVM_R = CW_SVM_R_Tune,
                                 CART = CART_Tune,
                                 CS_CART = CS_CART_Tune,
                                 C50 = C50_Tune,
                                 CS_C50 = CS_C50_Tune)))

##################################
# Creating a function model performance
# on test data
##################################
COST_COMPARISON_TEST_Specificity <- function(model, data) {
  Data_Test <- data.frame(Observed = data$Class,
                          Predicted = predict(model,
                                              data[,!names(data) %in% c("Class")],
                      type = "raw"))
  Specificity <- Specificity(y_pred = Data_Test$Predicted,
                             y_true = Data_Test$Observed)
  return(Specificity)
}


COST_COMPARISON_TEST_SUMMARY <- lapply(COST_COMPARISON_MODELS,
                                       COST_COMPARISON_TEST_Specificity,
                                       data = PMA_PreModelling_Test)
COST_COMPARISON_TEST_SUMMARY <- lapply(COST_COMPARISON_TEST_SUMMARY, as.vector)
COST_COMPARISON_TEST_SUMMARY <- do.call("rbind", COST_COMPARISON_TEST_SUMMARY)
colnames(COST_COMPARISON_TEST_SUMMARY) <- c("Specificity")
(COST_COMPARISON_TEST_SUMMARY <- as.data.frame(COST_COMPARISON_TEST_SUMMARY))

```

# **2. References**
|
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Reproducible Machine Learning for Credit Card Fraud Detection](https://bookdown.org/larget_jacob/data-modeling-methods/) by Yann-Aël Le Borgne, Wissam Siblini, Bertrand Lebichot and Gianluca Bontempi
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [rpart.plot](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) by Stephen Milborrow
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [pls](https://cran.r-project.org/web/packages/pls/pls.pdf) by Kristian Hovde Liland
| **[R Package]** [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) by Brian Ripley
| **[R Package]** [elasticnet](https://cran.r-project.org/web/packages/elasticnet/elasticnet.pdf) by Hui Zou
| **[R Package]** [earth](https://cran.r-project.org/web/packages/earth/earth.pdf) by Stephen Milborrow
| **[R Package]** [party](https://cran.r-project.org/web/packages/party/party.pdf) by Torsten Hothorn
| **[R Package]** [kernlab](https://cran.r-project.org/web/packages/kernlab/kernlab.pdf) by Alexandros Karatzoglou
| **[R Package]** [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) by Andy Liaw
| **[R Package]** [pROC](https://cran.r-project.org/web/packages/pROC/pROC.pdf) by Xavier Robin
| **[R Package]** [mda](https://cran.r-project.org/web/packages/mda/mda.pdf) by Trevor Hastie
| **[R Package]** [klaR](https://cran.r-project.org/web/packages/klaR/klaR.pdf) by Christian Roever, Nils Raabe, Karsten Luebke, Uwe Ligges, Gero Szepannek, Marc Zentgraf and David Meyer
| **[R Package]** [pamr](https://cran.r-project.org/web/packages/pamr/pamr.pdf) by Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan and Gil Chu
| **[Article]** [The caret Package](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[Article]** [A Short Introduction to the caret Package](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) by Max Kuhn
| **[Article]** [Caret Package – A Practical Guide to Machine Learning in R](https://www.machinelearningplus.com/machine-learning/caret-package/#:~:text=Caret%20is%20short%20for%20Classification%20And%20REgression%20Training.,track%20of%20which%20algorithm%20resides%20in%20which%20package.) by Selva Prabhakaran
| **[Article]** [Tuning Machine Learning Models Using the Caret R Package](https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/) by Jason Brownlee
| **[Article]** [Lattice Graphs](http://www.sthda.com/english/wiki/lattice-graphs) by Alboukadel Kassambara
| **[Article]** [A Tour of Machine Learning Algorithms](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/) by Jason Brownlee
| **[Article]** [Decision Tree Algorithm Examples In Data Mining](https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/) by Software Testing Help Team
| **[Article]** [4 Types of Classification Tasks in Machine Learning](https://machinelearningmastery.com/types-of-classification-in-machine-learning/) by Jason Brownlee
| **[Article]** [Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/) by Jason Brownlee
| **[Article]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Article]** [Machine Learning Tutorial: A Step-by-Step Guide for Beginners](http://www.feat.engineering/index.html) by Mayank Banoula
| **[Article]** [Classification Tree](https://support.bccvl.org.au/support/solutions/articles/6000083204-classification-tree) by BCCVL Team
| **[Article]** [C5.0: An Informal Tutorial](https://www.rulequest.com/see5-unix.html) by RuleQuest Team
| **[Article]** [Understand Support Vector Machine In Depth](https://iq.opengenus.org/understand-support-vector-machine-in-depth/) by IQ OpenGenus Team
| **[Article]** [A Gentle Introduction to Imbalanced Classification](https://machinelearningmastery.com/what-is-imbalanced-classification/) by Jason Brownlee
| **[Article]** [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://support.bccvl.org.au/support/solutions/articles/6000083212-generalized-boosting-model) by Jason Brownlee
| **[Article]** [Step-By-Step Framework for Imbalanced Classification Projects](https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/) by Jason Brownlee
| **[Article]** [How to Handle Imbalanced Classes in Machine Learning](https://elitedatascience.com/imbalanced-classes) by Elite Data Science Team
| **[Article]** [Best Ways To Handle Imbalanced Data in Machine Learning](https://dataaspirant.com/handle-imbalanced-data-machine-learning/) by Jaiganesh Nagidi
| **[Article]** [Handling Imbalanced Data for Classification](https://www.geeksforgeeks.org/handling-imbalanced-data-for-classification/) by Geeks For Geeks Team
| **[Article]** [Cost-Sensitive Learning for Imbalanced Classification](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/) by Jason Brownlee
| **[Article]** [Cost-Sensitive Learning](https://ml-concepts.com/2021/10/08/3-cost-sensitive-learning/) by Sourabh Gupta
| **[Article]** [Cost-Sensitive Classification](https://mlr.mlr-org.com/articles/tutorial/cost_sensitive_classif.html) by MLR Org Team
| **[Publication]** [Support Vector Regression Machines](http://www.cs.cmu.edu/~pakyan/compbio/references/Drucker_NIPS_1996.pdf) by Harris Drucker, Chris Burges, Linda Kaufman, Alex Smola and Vladimir Vapnik (Advances in Neural Information Processing Systems)
| **[Publication]** [Classification and Regression Trees](https://www.semanticscholar.org/paper/Classification-and-Regression-Trees-Breiman-Friedman/8017699564136f93af21575810d557dba1ee6fc6) by Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone (Computer Science)
| **[Publication]** [ C4.5: Programs for Machine Learning](https://link.springer.com/article/10.1007/BF00993309) by Ross Quinlan
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
| **[Course]** [Regression Methods](https://online.stat.psu.edu/stat501/) by Penn State Eberly College of Science
| **[Course]** [Applied Regression Analysis](https://online.stat.psu.edu/stat462/) by Penn State Eberly College of Science
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|